{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 환경 설정"
      ],
      "metadata": {
        "id": "CatmkL5wDus_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp==0.8.0\n",
        "!pip install tqdm pandas\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch>=1.8.1\n",
        "\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkAShrKQ8QwC",
        "outputId": "5bb5646d-b4c9-4611-9af1-e9faf5769141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2023.7.22)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n",
            "Collecting gluonnlp==0.8.0\n",
            "  Downloading gluonnlp-0.8.0.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0) (1.23.5)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.8.0-py3-none-any.whl size=292695 sha256=f7b50003d963e3537112434ea88538f77218b67e467cbbb184c9a1d1cbc532a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/cc/dc/7ec84dced25f738b8be400101abb67e4b50c905090a51017e4\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.8.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-9f4seaw9/kobert-tokenizer_74f2f3b5f4ff407bab91355aa60a7983\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-9f4seaw9/kobert-tokenizer_74f2f3b5f4ff407bab91355aa60a7983\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kobert_tokenizer\n",
            "  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4632 sha256=4bbcddb00a73d5189cc3f5df939be012b563fc334d4c8b48bce921553f2197d9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sh8tgdvd/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n",
            "Successfully built kobert_tokenizer\n",
            "Installing collected packages: kobert_tokenizer\n",
            "Successfully installed kobert_tokenizer-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mecab-python3\n",
        "!pip install konlpy\n",
        "!pip install tensorflow-text\n",
        "!pip install soynlp\n",
        "!pip install -v python-mecab-ko\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "r5pB917s8JsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c810c5f8-a0ac-46e4-d962-69015368ca03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.8\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (0.15.0)\n",
            "Collecting tensorflow<2.16,>=2.15.0 (from tensorflow-text)\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.59.2)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow<2.16,>=2.15.0->tensorflow-text)\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow-text)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow-text)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "Successfully installed keras-2.15.0 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-text-2.15.0\n",
            "Collecting soynlp\n",
            "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.23.5)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.2.0)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting python-mecab-ko\n",
            "  Downloading python_mecab_ko-1.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (573 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m573.9/573.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-mecab-ko-dic (from python-mecab-ko)\n",
            "  Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-mecab-ko-dic, python-mecab-ko\n",
            "Successfully installed python-mecab-ko-1.3.3 python-mecab-ko-dic-2.1.1.post2\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVxx7Ram9EVA",
        "outputId": "553f09ef-95ee-43f5-ef6a-39febc598247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R42whrqo8V6R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from konlpy.tag import Kkma\n",
        "from soynlp.normalizer import *\n",
        "from soynlp.tokenizer import LTokenizer\n",
        "from konlpy.tag import Komoran\n",
        "from transformers import BertTokenizer\n",
        "import mecab\n",
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "OGdgszhM89tI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe377bd-1d6e-43eb-a52f-169772973139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/mxnet/optimizer/optimizer.py:163: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  warnings.warn('WARNING: New optimizer %s.%s is overriding '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch GPU 설정\n",
        "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device_type)"
      ],
      "metadata": {
        "id": "k5O7Ftb18TLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터"
      ],
      "metadata": {
        "id": "H_E30NurD7jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_excel('/content/drive/MyDrive/WCRC/018.감성대화/Training_221115_add/원천데이터/감성대화말뭉치(최종데이터)_Training/감성대화말뭉치(최종데이터)_Training.xlsx')\n",
        "val = pd.read_excel('/content/drive/MyDrive/WCRC/018.감성대화/Validation_221115_add/원천데이터/감성대화말뭉치(최종데이터)_Validation/감성대화말뭉치(최종데이터)_Validation.xlsx')"
      ],
      "metadata": {
        "id": "2Rig7we39OWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "6PjuZpNWve1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df1 = train[['Unnamed: 0', '상황키워드', '감정_대분류', '감정_소분류', '사람문장1', '시스템문장1']]\n",
        "train_df2 = train[['Unnamed: 0', '상황키워드', '감정_대분류', '감정_소분류', '사람문장2', '시스템문장2']]\n",
        "train_df3 = train[['Unnamed: 0', '상황키워드', '감정_대분류', '감정_소분류', '사람문장3', '시스템문장3']]\n",
        "\n",
        "# 열 이름 변경\n",
        "train_df1 = train_df1.rename(columns={'시스템문장1': '시스템문장', '사람문장1': '사람문장'})\n",
        "train_df2 = train_df2.rename(columns={'시스템문장2': '시스템문장', '사람문장2': '사람문장'})\n",
        "train_df3 = train_df3.rename(columns={'시스템문장3': '시스템문장', '사람문장3': '사람문장'})\n",
        "\n",
        "train_df = pd.concat([train_df1, train_df2, train_df3], ignore_index=True)\n",
        "\n",
        "# NaN 값을 가진 행 삭제\n",
        "train_df = train_df.dropna(subset=['사람문장', '시스템문장'])\n",
        "\n",
        "# Unnamed: 0 기준으로 정렬\n",
        "train_df = train_df.sort_values(by='Unnamed: 0')\n",
        "\n",
        "# 인덱스 재설정\n",
        "train_df = train_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "8b9N44UKX-Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df1 = val[['Unnamed: 0', '상황키워드', '감정_대분류', '감정_소분류', '사람문장1', '시스템문장1']]\n",
        "val_df2 = val[['Unnamed: 0', '상황키워드', '감정_대분류', '감정_소분류', '사람문장2', '시스템문장2']]\n",
        "val_df3 = val[['Unnamed: 0', '상황키워드', '감정_대분류', '감정_소분류', '사람문장3', '시스템문장3']]\n",
        "\n",
        "val_df1 = val_df1.rename(columns={'시스템문장1': '시스템문장', '사람문장1': '사람문장'})\n",
        "val_df2 = val_df2.rename(columns={'시스템문장2': '시스템문장', '사람문장2': '사람문장'})\n",
        "val_df3 = val_df3.rename(columns={'시스템문장3': '시스템문장', '사람문장3': '사람문장'})\n",
        "\n",
        "val_df = pd.concat([val_df1, val_df2, val_df3], ignore_index=True)\n",
        "val_df = val_df.dropna(subset=['사람문장', '시스템문장'])\n",
        "val_df = val_df.sort_values(by='Unnamed: 0')\n",
        "val_df = val_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "OU6udmEGaS-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['상황키워드'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAtp4cVt5wUe",
        "outputId": "a4102187-3631-4419-a133-1c2e63ed07bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "대인관계            31376\n",
              "진로,취업,직장        14606\n",
              "재정              14584\n",
              "건강,죽음           12340\n",
              "가족관계            11768\n",
              "연애,결혼,출산        11282\n",
              "대인관계(부부, 자녀)     9641\n",
              "학업 및 진로          9020\n",
              "학교폭력/따돌림         8464\n",
              "건강               8452\n",
              "재정,은퇴,노후준비       7626\n",
              "직장, 업무 스트레스      6795\n",
              "Name: 상황키워드, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.loc[(train_df['상황키워드'] == \"대인관계\"), '상황키워드'] = 0\n",
        "train_df.loc[(train_df['상황키워드'] == \"진로,취업,직장\"), '상황키워드'] = 1\n",
        "train_df.loc[(train_df['상황키워드'] == \"재정\"), '상황키워드'] = 2\n",
        "train_df.loc[(train_df['상황키워드'] == \"건강,죽음\"), '상황키워드'] = 3\n",
        "train_df.loc[(train_df['상황키워드'] == \"가족관계\"), '상황키워드'] = 4\n",
        "train_df.loc[(train_df['상황키워드'] == \"연애,결혼,출산\"), '상황키워드'] = 5\n",
        "train_df.loc[(train_df['상황키워드'] == \"대인관계(부부, 자녀)\"), '상황키워드'] = 6\n",
        "train_df.loc[(train_df['상황키워드'] == \"학업 및 진로\"), '상황키워드'] = 7\n",
        "train_df.loc[(train_df['상황키워드'] == \"학교폭력/따돌림\"), '상황키워드'] = 8\n",
        "train_df.loc[(train_df['상황키워드'] == \"건강\"), '상황키워드'] = 9\n",
        "train_df.loc[(train_df['상황키워드'] == \"재정,은퇴,노후준비\"), '상황키워드'] = 10\n",
        "train_df.loc[(train_df['상황키워드'] == \"직장, 업무 스트레스\"), '상황키워드'] = 11"
      ],
      "metadata": {
        "id": "_IKfxs9i4suK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.loc[(val_df['상황키워드'] == \"대인관계\"), '상황키워드'] = 0\n",
        "val_df.loc[(val_df['상황키워드'] == \"진로,취업,직장\"), '상황키워드'] = 1\n",
        "val_df.loc[(val_df['상황키워드'] == \"재정\"), '상황키워드'] = 2\n",
        "val_df.loc[(val_df['상황키워드'] == \"건강,죽음\"), '상황키워드'] = 3\n",
        "val_df.loc[(val_df['상황키워드'] == \"가족관계\"), '상황키워드'] = 4\n",
        "val_df.loc[(val_df['상황키워드'] == \"연애,결혼,출산\"), '상황키워드'] = 5\n",
        "val_df.loc[(val_df['상황키워드'] == \"대인관계(부부, 자녀)\"), '상황키워드'] = 6\n",
        "val_df.loc[(val_df['상황키워드'] == \"학업 및 진로\"), '상황키워드'] = 7\n",
        "val_df.loc[(val_df['상황키워드'] == \"학교폭력/따돌림\"), '상황키워드'] = 8\n",
        "val_df.loc[(val_df['상황키워드'] == \"건강\"), '상황키워드'] = 9\n",
        "val_df.loc[(val_df['상황키워드'] == \"재정,은퇴,노후준비\"), '상황키워드'] = 10\n",
        "val_df.loc[(val_df['상황키워드'] == \"직장, 업무 스트레스\"), '상황키워드'] = 11"
      ],
      "metadata": {
        "id": "JnXDwNs5vWm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['감정_대분류'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXRfa_1e90LL",
        "outputId": "86ff7330-9c54-4575-e8da-04d185a06115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "불안    26301\n",
              "분노    25999\n",
              "상처    25957\n",
              "슬픔    25814\n",
              "당황    24936\n",
              "기쁨    16947\n",
              "Name: 감정_대분류, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.loc[(train_df['감정_대분류'] == \"불안\"), '감정_대분류'] = 0\n",
        "train_df.loc[(train_df['감정_대분류'] == \"분노\"), '감정_대분류'] = 1\n",
        "train_df.loc[(train_df['감정_대분류'] == \"상처\"), '감정_대분류'] = 2\n",
        "train_df.loc[(train_df['감정_대분류'] == \"슬픔\"), '감정_대분류'] = 3\n",
        "train_df.loc[(train_df['감정_대분류'] == \"당황\"), '감정_대분류'] = 4\n",
        "train_df.loc[(train_df['감정_대분류'] == \"기쁨\"), '감정_대분류'] = 5"
      ],
      "metadata": {
        "id": "eqLGJWpv-2Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.loc[(val_df['감정_대분류'] == \"불안\"), '감정_대분류'] = 0\n",
        "val_df.loc[(val_df['감정_대분류'] == \"분노\"), '감정_대분류'] = 1\n",
        "val_df.loc[(val_df['감정_대분류'] == \"상처\"), '감정_대분류'] = 2\n",
        "val_df.loc[(val_df['감정_대분류'] == \"슬픔\"), '감정_대분류'] = 3\n",
        "val_df.loc[(val_df['감정_대분류'] == \"당황\"), '감정_대분류'] = 4\n",
        "val_df.loc[(val_df['감정_대분류'] == \"기쁨\"), '감정_대분류'] = 5"
      ],
      "metadata": {
        "id": "j-terL8P_Ijx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "def clean_korean_documents(documents):\n",
        "    cleaned_documents = []\n",
        "\n",
        "    for document in documents:\n",
        "        okt = Okt()\n",
        "        clean_words = []\n",
        "\n",
        "        for word in okt.pos(document, stem=True):  # 어간 추출\n",
        "            if word[1] in ['Noun', 'Verb', 'Adjective']:  # 명사, 동사, 형용사\n",
        "                clean_words.append(word[0])\n",
        "\n",
        "        cleaned_document = ' '.join(clean_words)\n",
        "        cleaned_documents.append(cleaned_document)\n",
        "\n",
        "    '''\n",
        "    # 불용어 제거\n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', '의']  # 예제로 일부 불용어 추가\n",
        "    cleaned_documents = [' '.join([word for word in document.split() if word not in stopwords]) for document in cleaned_documents]\n",
        "    '''\n",
        "\n",
        "    return cleaned_documents\n",
        "\n",
        "# '사람문장' 열에 대해 텍스트 정제 적용\n",
        "train_df['사람문장_텍스트'] = clean_korean_documents(train_df['사람문장'])\n",
        "val_df['사람문장_텍스트'] = clean_korean_documents(val_df['사람문장'])"
      ],
      "metadata": {
        "id": "O4_wdAAt8RlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "# 형태소 분석기 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# '사람문장_텍스트' 열에서 명사 추출하여 새로운 열 생성\n",
        "train_df['사람문장_명사'] = train_df['사람문장'].apply(lambda x: ' '.join([word for word in okt.nouns(x)]))\n",
        "val_df['사람문장_명사'] = val_df['사람문장'].apply(lambda x: ' '.join([word for word in okt.nouns(x)]))"
      ],
      "metadata": {
        "id": "Ssv_X0rT35Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# '사람문장' 열에서 TF-IDF 벡터화\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(train_df['사람문장'])\n",
        "\n",
        "# 각 단어의 중요도를 나타내는 특성 이름 가져오기\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# 중요한 단어 추출 (상위 5개)\n",
        "top_5_important_words = []\n",
        "for i, row in train_df.iterrows():\n",
        "    tfidf_scores = zip(feature_names, tfidf_matrix[i].toarray()[0])\n",
        "    sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
        "    top_5_important_words.append([word for word, score in sorted_tfidf_scores[:5]])\n",
        "\n",
        "# 결과를 데이터프레임에 추가\n",
        "train_df['상위5중요단어'] = top_5_important_words"
      ],
      "metadata": {
        "id": "XwNmutFEvWxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# '사람문장' 열에서 TF-IDF 벡터화\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(val_df['사람문장'])\n",
        "\n",
        "# 각 단어의 중요도를 나타내는 특성 이름 가져오기\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# 중요한 단어 추출 (상위 5개)\n",
        "top_5_important_words = []\n",
        "for i, row in val_df.iterrows():\n",
        "    tfidf_scores = zip(feature_names, tfidf_matrix[i].toarray()[0])\n",
        "    sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
        "    top_5_important_words.append([word for word, score in sorted_tfidf_scores[:5]])\n",
        "\n",
        "# 결과를 데이터프레임에 추가\n",
        "val_df['상위5중요단어'] = top_5_important_words"
      ],
      "metadata": {
        "id": "cjkoPIWJYP7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 열의 값을 띄어쓰기로 연결하여 새로운 열 생성\n",
        "train_df['상위5중요단어'] = train_df['상위5중요단어'].apply(lambda x: ' '.join(x))\n",
        "val_df['상위5중요단어'] = val_df['상위5중요단어'].apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "Ju4TktzIXAOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "# 형태소 분석기 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# '상위5중요단어' 열의 값을 가져와서 명사만 추출하여 '상위5중요단어_명사' 열에 저장\n",
        "train_df['상위5중요단어_명사'] = train_df['상위5중요단어'].apply(lambda x: ' '.join([word for word in okt.nouns(x)]))\n",
        "val_df['상위5중요단어_명사'] = val_df['상위5중요단어'].apply(lambda x: ' '.join([word for word in okt.nouns(x)]))"
      ],
      "metadata": {
        "id": "1i_6q-PaRCfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv('/content/drive/MyDrive/WCRC/final_train_df.csv', index=False)\n",
        "val_df.to_csv('/content/drive/MyDrive/WCRC/final_val_df.csv', index=False)"
      ],
      "metadata": {
        "id": "iHS70LXlH357"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/WCRC/final_train_df.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/WCRC/final_val_df.csv')"
      ],
      "metadata": {
        "id": "_ga7WDrI7yCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}